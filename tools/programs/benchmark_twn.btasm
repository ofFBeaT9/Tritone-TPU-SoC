# Benchmark: Ternary Weight Network (TWN) Inference
# Purpose: Neural network workload with {-1, 0, +1} weights
# Metrics: MAC throughput, activation computation, memory efficiency
#
# Implements a small 2-layer fully-connected network:
#   Input:  4 neurons (balanced ternary values)
#   Hidden: 4 neurons (with ternary weights, ReLU activation)
#   Output: 2 neurons (with ternary weights)
#
# TWN advantage: With weights âˆˆ {-1, 0, +1}:
#   - Multiply becomes: NEG (w=-1), skip (w=0), pass (w=1)
#   - No actual multiplication hardware needed
#   - Perfect match for balanced ternary arithmetic
#
# This is the key use case for Tritone: efficient neural network inference

# ============================================================
# Memory Layout
# ============================================================
# Address 0-3:    Input vector (4 values)
# Address 4-19:   Layer 1 weights (4x4 = 16 ternary weights)
# Address 20-23:  Layer 1 output / Hidden activation (4 values)
# Address 24-31:  Layer 2 weights (4x2 = 8 ternary weights)
# Address 32-33:  Output vector (2 values)

# ============================================================
# Initialize Input Vector
# ============================================================
    LDI R1, 3           # input[0] = 3
    ST R1, R0, 0
    LDI R1, -2          # input[1] = -2
    ST R1, R0, 1
    LDI R1, 1           # input[2] = 1
    ST R1, R0, 2
    LDI R1, 4           # input[3] = 4
    ST R1, R0, 3

# ============================================================
# Initialize Layer 1 Weights (4x4 matrix, ternary)
# ============================================================
# Weight matrix W1 (row-major):
#   [ +1  -1  +1   0 ]   neurons 0
#   [  0  +1  -1  +1 ]   neurons 1
#   [ -1   0  +1  +1 ]   neurons 2
#   [ +1  +1   0  -1 ]   neurons 3

    # Row 0: [+1, -1, +1, 0]
    LDI R1, 1
    ST R1, R0, 4        # W1[0,0] = +1
    LDI R1, -1
    ST R1, R0, 5        # W1[0,1] = -1
    LDI R1, 1
    ST R1, R0, 6        # W1[0,2] = +1
    LDI R1, 0
    ST R1, R0, 7        # W1[0,3] = 0

    # Row 1: [0, +1, -1, +1]
    LDI R1, 0
    ST R1, R0, 8
    LDI R1, 1
    ST R1, R0, 9
    LDI R1, -1
    ST R1, R0, 10
    LDI R1, 1
    ST R1, R0, 11

    # Row 2: [-1, 0, +1, +1]
    LDI R1, -1
    ST R1, R0, 12
    LDI R1, 0
    ST R1, R0, 13
    LDI R1, 1
    ST R1, R0, 14
    LDI R1, 1
    ST R1, R0, 15

    # Row 3: [+1, +1, 0, -1]
    LDI R1, 1
    ST R1, R0, 16
    LDI R1, 1
    ST R1, R0, 17
    LDI R1, 0
    ST R1, R0, 18
    LDI R1, -1
    ST R1, R0, 19

# ============================================================
# Layer 1: Compute Hidden Activations
# ============================================================
# h[i] = ReLU(sum(W1[i,j] * input[j]))
# With ternary weights, this becomes ADD/SUB operations

# Load inputs once (reuse for all neurons)
    LD R1, R0, 0        # R1 = input[0] = 3
    LD R2, R0, 1        # R2 = input[1] = -2
    LD R3, R0, 2        # R3 = input[2] = 1
    LD R4, R0, 3        # R4 = input[3] = 4

# Hidden neuron 0: h[0] = +1*3 + (-1)*(-2) + (+1)*1 + 0*4 = 3 + 2 + 1 = 6
    ADD R5, R1, R0      # R5 = +1 * input[0] = 3
    SUB R5, R5, R2      # R5 = 3 - input[1] = 3 - (-2) = 5
    ADD R5, R5, R3      # R5 = 5 + input[2] = 6
    # (skip input[3] since weight = 0)
    # ReLU: if R5 < 0 then R5 = 0 (but R5 = 6 > 0, so keep)
    ST R5, R0, 20       # h[0] = 6

# Hidden neuron 1: h[1] = 0*3 + (+1)*(-2) + (-1)*1 + (+1)*4 = -2 - 1 + 4 = 1
    ADD R5, R2, R0      # R5 = input[1] = -2
    SUB R5, R5, R3      # R5 = -2 - input[2] = -3
    ADD R5, R5, R4      # R5 = -3 + input[3] = 1
    # ReLU: R5 = 1 > 0, keep
    ST R5, R0, 21       # h[1] = 1

# Hidden neuron 2: h[2] = (-1)*3 + 0*(-2) + (+1)*1 + (+1)*4 = -3 + 1 + 4 = 2
    NEG R5, R1          # R5 = -input[0] = -3
    ADD R5, R5, R3      # R5 = -3 + input[2] = -2
    ADD R5, R5, R4      # R5 = -2 + input[3] = 2
    # ReLU: R5 = 2 > 0, keep
    ST R5, R0, 22       # h[2] = 2

# Hidden neuron 3: h[3] = (+1)*3 + (+1)*(-2) + 0*1 + (-1)*4 = 3 - 2 - 4 = -3
    ADD R5, R1, R2      # R5 = input[0] + input[1] = 1
    SUB R5, R5, R4      # R5 = 1 - input[3] = -3
    # ReLU: R5 = -3 < 0, so R5 = 0
    LDI R6, 0
    # Simple ReLU: check if negative
    BLT R5, R6, relu_zero_h3
    ST R5, R0, 23
    JR R0, 2            # Skip next instruction
relu_zero_h3:
    ST R6, R0, 23       # h[3] = 0 (after ReLU)

# ============================================================
# Layer 2: Compute Output (2 neurons)
# ============================================================
# Initialize Layer 2 weights (4x2 matrix)
# W2 = [ +1  -1 ]
#      [ -1  +1 ]
#      [ +1   0 ]
#      [  0  +1 ]

    LDI R1, 1
    ST R1, R0, 24       # W2[0,0] = +1
    LDI R1, -1
    ST R1, R0, 25       # W2[0,1] = -1
    LDI R1, -1
    ST R1, R0, 26       # W2[1,0] = -1
    LDI R1, 1
    ST R1, R0, 27       # W2[1,1] = +1
    LDI R1, 1
    ST R1, R0, 28       # W2[2,0] = +1
    LDI R1, 0
    ST R1, R0, 29       # W2[2,1] = 0
    LDI R1, 0
    ST R1, R0, 30       # W2[3,0] = 0
    LDI R1, 1
    ST R1, R0, 31       # W2[3,1] = +1

# Load hidden activations
    LD R1, R0, 20       # h[0] = 6
    LD R2, R0, 21       # h[1] = 1
    LD R3, R0, 22       # h[2] = 2
    LD R4, R0, 23       # h[3] = 0

# Output neuron 0: o[0] = (+1)*6 + (-1)*1 + (+1)*2 + 0*0 = 6 - 1 + 2 = 7
    ADD R5, R1, R0      # R5 = h[0] = 6
    SUB R5, R5, R2      # R5 = 6 - h[1] = 5
    ADD R5, R5, R3      # R5 = 5 + h[2] = 7
    ST R5, R0, 32       # o[0] = 7

# Output neuron 1: o[1] = (-1)*6 + (+1)*1 + 0*2 + (+1)*0 = -6 + 1 = -5
    NEG R5, R1          # R5 = -h[0] = -6
    ADD R5, R5, R2      # R5 = -6 + h[1] = -5
    ADD R5, R5, R4      # R5 = -5 + h[3] = -5
    ST R5, R0, 33       # o[1] = -5

# ============================================================
# Validation
# ============================================================
# Expected outputs: o[0] = 7, o[1] = -5

    LD R1, R0, 32       # Load o[0]
    LDI R2, 7
    BEQ R1, R2, check_o0_pass
    LDI R8, -1
    HALT

check_o0_pass:
    LD R1, R0, 33       # Load o[1]
    LDI R2, -5
    BEQ R1, R2, check_o1_pass
    LDI R8, -2
    HALT

check_o1_pass:
    LDI R8, 42          # Success: TWN inference complete
    HALT

# ============================================================
# Performance Notes
# ============================================================
# This TWN benchmark demonstrates:
# 1. Ternary weights eliminate all multiplications
# 2. Natural fit for balanced ternary arithmetic
# 3. Memory-efficient weight storage (log2(3) bits per weight)
# 4. Dual-issue potential in matrix-vector operations
#
# For publication:
# - Compare MAC/cycle against binary implementation
# - Measure energy per inference
# - Quantify memory bandwidth savings from ternary weights
